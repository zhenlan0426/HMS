{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from torch.optim import AdamW,Adam,SGD\n",
    "import math\n",
    "LOCAL = True\n",
    "if LOCAL:\n",
    "    dataPath = '/home/zhenlan/Desktop/Projects/HMS/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP\n",
    "config = {\n",
    "    \"d_model\": 512,\n",
    "    \"n_layer\": 12,\n",
    "    \"rms_norm\": False,\n",
    "    \"normOverChannel\": True,\n",
    "    \"in_channels\": 20, # if EEG\n",
    "    \"seqLen\": 256,\n",
    "    \"dataFolder\": \"train_eegs_LocalNorm\",\n",
    "    # \"classWeight\": 0.0\n",
    "}\n",
    "\n",
    "config.update(fixed_config)\n",
    "config = Config(config)\n",
    "\n",
    "# other training parameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "accumulation_steps = 2\n",
    "lr = 6e-5\n",
    "clip = 6e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(dataPath+'train_data.pkl')\n",
    "val = pd.read_pickle(dataPath+'val_data.pkl')\n",
    "# trainData = eegData(train,dataPath,config.dataFolder,seqLen=config.seqLen)\n",
    "# valData = eegData(val,dataPath,config.dataFolder,seqLen=config.seqLen)\n",
    "trainData = eegDataXonly(train,dataPath,config.dataFolder,seqLen=config.seqLen)\n",
    "valData = eegDataXonly(val,dataPath,config.dataFolder,seqLen=config.seqLen)\n",
    "\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seqModel(config).to('cuda')\n",
    "trainable_params = list(model.parameters())\n",
    "optimizer = AdamW(trainable_params,lr = lr,amsgrad=True)\n",
    "# optimizer = Adam(trainable_params,lr = lr)\n",
    "# optimizer = SGD(trainable_params,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (input_ids,targets) in enumerate(train_loader):\n",
    "#         input_ids,targets = input_ids.to('cuda'),targets.to('cuda')\n",
    "#         autoL,classL = model(input_ids,targets)\n",
    "#         loss = autoL + config.classWeight * classL\n",
    "#         loss.backward()\n",
    "#         break\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     if torch.any(torch.isnan(param.grad)):\n",
    "#         print('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 0.5643352270126343, eval loss 0.5241860747337341\n",
      "epoch 1: train loss 0.5055997371673584, eval loss 0.5055713057518005\n",
      "epoch 2: train loss 0.49228569865226746, eval loss 0.48613426089286804\n",
      "epoch 3: train loss 0.4745284616947174, eval loss 0.45736682415008545\n",
      "epoch 4: train loss 0.465613454580307, eval loss 0.46564626693725586\n",
      "epoch 5: train loss 0.45951899886131287, eval loss 0.45764753222465515\n",
      "epoch 6: train loss 0.4544704556465149, eval loss 0.45245009660720825\n",
      "epoch 7: train loss 0.4458999037742615, eval loss 0.4487306773662567\n",
      "epoch 8: train loss 0.4407556354999542, eval loss 0.4432801306247711\n",
      "epoch 9: train loss 0.4364316463470459, eval loss 0.4364400804042816\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, input_ids in enumerate(train_loader):\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        loss = model(input_ids)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, input_ids in enumerate(val_loader):\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                loss = model(input_ids)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust seqLen\n",
    "trainData.seqLen = int(trainData.seqLen * 2)\n",
    "valData.seqLen = int(trainData.seqLen * 2)\n",
    "batch_size = int(batch_size / 2)\n",
    "# accumulation_steps = int(accumulation_steps * 2)\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 0.423208087682724, eval loss 0.4182685315608978\n",
      "epoch 1: train loss 0.4170055389404297, eval loss 0.4170435965061188\n",
      "epoch 2: train loss 0.41220834851264954, eval loss 0.40758243203163147\n",
      "epoch 3: train loss 0.4099055528640747, eval loss 0.4065611660480499\n",
      "epoch 4: train loss 0.4033583998680115, eval loss 0.4019213914871216\n",
      "epoch 5: train loss 0.4022318720817566, eval loss 0.3959902822971344\n",
      "epoch 6: train loss 0.39832955598831177, eval loss 0.4028504490852356\n",
      "epoch 7: train loss 0.39300668239593506, eval loss 0.3979494869709015\n",
      "epoch 8: train loss 0.39111995697021484, eval loss 0.39449262619018555\n",
      "epoch 9: train loss 0.3881307542324066, eval loss 0.3912297487258911\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, input_ids in enumerate(train_loader):\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        loss = model(input_ids)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, input_ids in enumerate(val_loader):\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                loss = model(input_ids)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust seqLen\n",
    "trainData.seqLen = int(trainData.seqLen * 2)\n",
    "valData.seqLen = int(trainData.seqLen * 2)\n",
    "batch_size = int(batch_size / 2)\n",
    "# accumulation_steps = int(accumulation_steps * 2)\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 0.3890700340270996, eval loss 0.39127740263938904\n",
      "epoch 1: train loss 0.37898337841033936, eval loss 0.37718817591667175\n",
      "epoch 2: train loss 0.3727087080478668, eval loss 0.3745807707309723\n",
      "epoch 3: train loss 0.3680777847766876, eval loss 0.3733740448951721\n",
      "epoch 4: train loss 0.3635190427303314, eval loss 0.3750753104686737\n",
      "epoch 5: train loss 0.35890939831733704, eval loss 0.3664422929286957\n",
      "epoch 6: train loss 0.3572143018245697, eval loss 0.36590519547462463\n",
      "epoch 7: train loss 0.35246413946151733, eval loss 0.3540668487548828\n",
      "epoch 8: train loss 0.3504982888698578, eval loss 0.3626132607460022\n",
      "epoch 9: train loss 0.347641259431839, eval loss 0.35376644134521484\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, input_ids in enumerate(train_loader):\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        loss = model(input_ids)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, input_ids in enumerate(val_loader):\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                loss = model(input_ids)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust seqLen\n",
    "trainData.seqLen = int(trainData.seqLen * 2)\n",
    "valData.seqLen = int(trainData.seqLen * 2)\n",
    "batch_size = int(batch_size / 2)\n",
    "# accumulation_steps = int(accumulation_steps * 2)\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 0.3441787362098694, eval loss 0.3506990671157837\n",
      "epoch 1: train loss 0.33828991651535034, eval loss 0.35045817494392395\n",
      "epoch 2: train loss 0.33368730545043945, eval loss 0.34320536255836487\n",
      "epoch 3: train loss 0.328596830368042, eval loss 0.3480270802974701\n",
      "epoch 4: train loss 0.32515719532966614, eval loss 0.33498314023017883\n",
      "epoch 5: train loss 0.3217135965824127, eval loss 0.3361016511917114\n",
      "epoch 6: train loss 0.31889957189559937, eval loss 0.33484527468681335\n",
      "epoch 7: train loss 0.3152419626712799, eval loss 0.3323552906513214\n",
      "epoch 8: train loss 0.3136831820011139, eval loss 0.32940948009490967\n",
      "epoch 9: train loss 0.3090258836746216, eval loss 0.3315391540527344\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, input_ids in enumerate(train_loader):\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        loss = model(input_ids)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, input_ids in enumerate(val_loader):\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                loss = model(input_ids)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_name = 'model1'\n",
    "torch.save(model.state_dict(), '/home/zhenlan/Desktop/Projects/HMS/Model_state/'+model_name+'.pth')\n",
    "import pickle\n",
    "with open('/home/zhenlan/Desktop/Projects/HMS/Model_state/'+model_name+'.pkl', 'wb') as file:\n",
    "    pickle.dump(config, file)\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(dataPath+'train_data.pkl')\n",
    "val = pd.read_pickle(dataPath+'val_data.pkl')\n",
    "trainData = eegDataXonly(train,dataPath,config.dataFolder,seqLen=10000)\n",
    "valData = eegDataXonly(val,dataPath,config.dataFolder,seqLen=10000)\n",
    "batch_size = 1\n",
    "accumulation_steps = 4\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model_name = 'model1'\n",
    "model.load_state_dict(torch.load('/home/zhenlan/Desktop/Projects/HMS/Model_state/'+model_name+'.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust seqLen\n",
    "# trainData.seqLen = 10000\n",
    "# valData.seqLen = 10000\n",
    "# batch_size = 1\n",
    "# accumulation_steps = 4\n",
    "# train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "# val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 0.3123624324798584, eval loss 0.328789085149765\n",
      "epoch 1: train loss 0.29967764019966125, eval loss 0.3213753402233124\n",
      "epoch 2: train loss 0.2900429368019104, eval loss 0.3142571449279785\n",
      "epoch 3: train loss 0.28209131956100464, eval loss 0.31462356448173523\n",
      "epoch 4: train loss 0.27488476037979126, eval loss 0.3100687563419342\n",
      "epoch 5: train loss 0.26925671100616455, eval loss 0.30670639872550964\n",
      "epoch 6: train loss 0.2631770074367523, eval loss 0.3039543330669403\n",
      "epoch 7: train loss 0.258156955242157, eval loss 0.3038141131401062\n",
      "epoch 8: train loss 0.25356540083885193, eval loss 0.3010229170322418\n",
      "epoch 9: train loss 0.2493651658296585, eval loss 0.3004760146141052\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, input_ids in enumerate(train_loader):\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        loss = model(input_ids)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, input_ids in enumerate(val_loader):\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                loss = model(input_ids)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "batch_size = 2\n",
    "accumulation_steps = 4\n",
    "lr = 6e-6\n",
    "clip = 6e-4\n",
    "train = pd.read_pickle(dataPath+'train_data.pkl')\n",
    "val = pd.read_pickle(dataPath+'val_data.pkl')\n",
    "trainData = eegData(train,dataPath,config.dataFolder,seqLen=10000)\n",
    "valData = eegData(val,dataPath,config.dataFolder,seqLen=10000)\n",
    "\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(trainable_params,lr = lr,amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 1.1511163711547852, eval loss 0.9513724446296692\n",
      "epoch 1: train loss 0.8458009958267212, eval loss 0.8123239874839783\n",
      "epoch 2: train loss 0.7216739058494568, eval loss 0.762212336063385\n",
      "epoch 3: train loss 0.6343607306480408, eval loss 0.7389225959777832\n",
      "epoch 4: train loss 0.5664060711860657, eval loss 0.7422981858253479\n",
      "epoch 5: train loss 0.511198878288269, eval loss 0.7324237823486328\n",
      "epoch 6: train loss 0.4618319571018219, eval loss 0.7608739733695984\n",
      "epoch 7: train loss 0.41717249155044556, eval loss 0.762435257434845\n",
      "epoch 8: train loss 0.36739709973335266, eval loss 0.8094151020050049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(input_ids,target)\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m     clip_grad_value_(trainable_params,clip)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, (input_ids,target) in enumerate(train_loader):\n",
    "        input_ids,target = input_ids.to('cuda'), target.to('cuda')\n",
    "        loss = model(input_ids,target)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids,target) in enumerate(val_loader):\n",
    "                input_ids,target = input_ids.to('cuda'), target.to('cuda')\n",
    "                loss = model(input_ids,target)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train from start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "batch_size = 2\n",
    "accumulation_steps = 4\n",
    "lr = 6e-6\n",
    "clip = 6e-4\n",
    "train = pd.read_pickle(dataPath+'train_data.pkl')\n",
    "val = pd.read_pickle(dataPath+'val_data.pkl')\n",
    "trainData = eegData(train,dataPath,config.dataFolder,seqLen=10000)\n",
    "valData = eegData(val,dataPath,config.dataFolder,seqLen=10000)\n",
    "\n",
    "train_loader = DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(valData, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seqModel(config).to('cuda')\n",
    "trainable_params = list(model.parameters())\n",
    "optimizer = AdamW(trainable_params,lr = lr,amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train loss 1.308092713356018, eval loss 1.23896062374115\n",
      "epoch 1: train loss 1.2475287914276123, eval loss 1.2214336395263672\n",
      "epoch 2: train loss 1.2331287860870361, eval loss 1.209027886390686\n",
      "epoch 3: train loss 1.2163844108581543, eval loss 1.1937953233718872\n",
      "epoch 4: train loss 1.1912814378738403, eval loss 1.1904999017715454\n",
      "epoch 5: train loss 1.1722325086593628, eval loss 1.1823939085006714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(input_ids,target)\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m     clip_grad_value_(trainable_params,clip)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # train\n",
    "    for i, (input_ids,target) in enumerate(train_loader):\n",
    "        input_ids,target = input_ids.to('cuda'), target.to('cuda')\n",
    "        loss = model(input_ids,target)\n",
    "        loss.backward()\n",
    "        train_loss += loss.detach().cpu()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            clip_grad_value_(trainable_params,clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(i,train_loss)\n",
    "    train_loss /= (i+1)\n",
    "\n",
    "    # eval\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (input_ids,target) in enumerate(val_loader):\n",
    "                input_ids,target = input_ids.to('cuda'), target.to('cuda')\n",
    "                loss = model(input_ids,target)\n",
    "                eval_loss += loss.detach().cpu()\n",
    "    eval_loss /= (i+1)\n",
    "    print(f\"epoch {epoch}: train loss {train_loss}, eval loss {eval_loss}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
